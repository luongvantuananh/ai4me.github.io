<!DOCTYPE html>
<html>
  <head>
    <title>Nhận Dạng Chữ Tiếng Việt - Vietnamese OCR – Tuan Anh – AI Engineer/ Data Scientist</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Giới thiệu
Nhận dạng chữ viết tay là một trong những bài toán rất thú vị, với đầu vào là một ảnh chứa chữ và đầu ra là chữ chứa trong ảnh đó. Bài toán này tương đối khó đối với chữ viết tay, cộng với viết bộ dữ liệu việt nam tương đối hiếm có. Lần này, mình sẽ chia sẽ hướng tiếp cận mà team mình đã sử dụng trong cuộc thi do Cinnamon tổ chức, nhờ những cách sử lý hợp lý mà team mình đã đạt đượt top 1 trong phase 1 của cuộc thi này. Nhưng mà cuộc thì này tổ chức cũng dài hơi, làm những thí sinh hơi nản. (tầm 2 tháng với phần thi + bootcamp). Sơ qua về một chút ứng dụng của OCR. Mình thấy có 2 ứng dụng khá nổi tiếng là Google Translate, với GotIt của Hùng Trần.

" />
    <meta property="og:description" content="Giới thiệu
Nhận dạng chữ viết tay là một trong những bài toán rất thú vị, với đầu vào là một ảnh chứa chữ và đầu ra là chữ chứa trong ảnh đó. Bài toán này tương đối khó đối với chữ viết tay, cộng với viết bộ dữ liệu việt nam tương đối hiếm có. Lần này, mình sẽ chia sẽ hướng tiếp cận mà team mình đã sử dụng trong cuộc thi do Cinnamon tổ chức, nhờ những cách sử lý hợp lý mà team mình đã đạt đượt top 1 trong phase 1 của cuộc thi này. Nhưng mà cuộc thì này tổ chức cũng dài hơi, làm những thí sinh hơi nản. (tầm 2 tháng với phần thi + bootcamp). Sơ qua về một chút ứng dụng của OCR. Mình thấy có 2 ứng dụng khá nổi tiếng là Google Translate, với GotIt của Hùng Trần.

" />
    
    <meta name="author" content="Tuan Anh" />

    
    <meta property="og:title" content="Nhận Dạng Chữ Tiếng Việt - Vietnamese OCR" />
    <meta property="twitter:title" content="Nhận Dạng Chữ Tiếng Việt - Vietnamese OCR" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Tuan Anh - AI Engineer/ Data Scientist" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="/images/luffy.jpeg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Tuan Anh</a></h1>
            <p class="site-description">AI Engineer/ Data Scientist</p>
          </div>







          <nav>


            <a href="/"><b>Blog</b></a>
<!--            <a href="/datascientist"><b>Data Scientist</b></a>-->
<!--            <a href="/deeplearning"><b>Deep Learning</b></a>-->
            <a href="/tailieu"><b>Tài Liệu</b></a>
            <a href="/about"><b>About</b></a>
            <a href="/resume"><b>Resume</b></a>
          </nav>


        </header>
        <div>
          <nav>
              <a href="/deeplearning"><b style="color:blue;">Deep Learning</b></a>
              <a href="/computervision"><b style="color:blue;">Computer Vision</b></a>
              <a href="/bigdata"><b style="color:blue;">Big Data</b></a>
          </nav>
        </div>
        <br>
        <br>
        <br>

      </div>

    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Nhận Dạng Chữ Tiếng Việt - Vietnamese OCR</h1>

  <div class="entry">
    <h3 id="giới-thiệu">Giới thiệu</h3>
<p>Nhận dạng chữ viết tay là một trong những bài toán rất thú vị, với đầu vào là một ảnh chứa chữ và đầu ra là chữ chứa trong ảnh đó. Bài toán này tương đối khó đối với chữ viết tay, cộng với viết bộ dữ liệu việt nam tương đối hiếm có. Lần này, mình sẽ chia sẽ hướng tiếp cận mà team mình đã sử dụng trong cuộc thi do Cinnamon tổ chức, nhờ những cách sử lý hợp lý mà team mình đã đạt đượt top 1 trong phase 1 của cuộc thi này. Nhưng mà cuộc thì này tổ chức cũng dài hơi, làm những thí sinh hơi nản. (tầm 2 tháng với phần thi + bootcamp). Sơ qua về một chút ứng dụng của OCR. Mình thấy có 2 ứng dụng khá nổi tiếng là Google Translate, với GotIt của Hùng Trần.</p>

<h3 id="dữ-liệu">Dữ liệu</h3>
<p>Dữ liệu của bài toán này là chỉ có text-line không có scene text nên vấn đề đơn giản hơn nhiều so với việc phải detect được chữ trên ảnh có background như ngoài thực tế. 
<img src="/images/ocr_dataset.png" alt="dữ liệu" /></p>

<h2 id="chuẩn-bị-dữ-liệu">Chuẩn bị dữ liệu</h2>
<p>Đối với văn bản scan thì việc tiền xử lý như remove noise,background là cực kì quan trọng và ảnh hưởng khá nhiểu đến độ chính xác của bài toán. Để remove noise và background thì các bạn có thể sử dụng kmean để cluster bức ảnh ra 2 màu chủ đạo trắng và đen rồi sau đó binary ảnh dựa vào kết quả cluster. Trong quá trình xử lý các bạn có thể sử dụng <a href="https://github.com/mauvilsa/imgtxtenh">tool</a> này nhé</p>

<div class="img-div">
    <img src="/images/ocr_process_step.png" />
</div>

<h2 id="mô-hình">Mô hình</h2>
<p>Dữ liệu đã chuẩn bị xong thì đến phần model. Môt trong những mô hình được hay sử dụng là CRNN, tuy nhiên mô hình mình sử dụng thêm vào cơ chế attention cho phép model lựa chọn vùng ảnh mong muốn để phát sinh ra text. Cơ chế attention được sử dụng rất nhiều trong machine translation. Mình sẽ có một bài về cơ chế này,tuy nhiên các bạn có thể tham khảo <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">tại đây</a>.</p>

<div class="img-div">
    <img src="/images/ocr_crnn.png" />
</div>

<p>Đối dữ liệu là ảnh thì chúng ta sẽ dùng mô hình CNN để extract feature.  Ở đây, mình dùng VGG16 nhé. Trong model hình này, chúng ta nên lưu ý số tầng pooling, mình chỉ sử dụng 4 tầng pooling của VGG16, mỗi tầng pooling sẽ có kích thước 2x2,đồng thời bỏ hết tất cả các tầng fully connected cuối cùng, do đó output của VGG16 là một tập các feature maps, mỗi pixel trên feature tương ứng vùng 16x16 trên bức ảnh đầu vào.</p>

<div class="img-div">
    <img src="/images/ocr_pooling_size.png" />
</div>

<p>Việc chọn kích thước và số tầng pooling này cực kì quan trọng vì nó ảnh hưởng đến số pixel mà mỗi timestep nhìn thấy được.Nếu các bạn chọn kính thước tầng pooling size quá lớn sẽ dần đến việc một step sẽ bao gồm nhiểũ chữ trong ảnh do đó mô hình sẽ không nhận dạng được.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">base_model</span> <span class="o">=</span> <span class="n">applications</span><span class="p">.</span><span class="n">VGG16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>Với ảnh đầu vào có kích thước 1280x60 thì output của vgg16 là (nmaps, w, h) = …, mỗi dòng tương ứng với chiều w thì tương ứng với một timstep cho tầng LSTM.</p>

<h3 id="visual-attention">Visual attention</h3>
<p>Với mô hình CRNN, kết quả của vgg được truyền trực tiếp vào mô hình LSTM, tuy nhiên, với thực nghiệp của mình khi stack thêm một lớp attention ở giữa tầng vgg và LSTM sẽ cho kết quả nhận dạng tốt hơn. Attention cho phép model của chúng ta được thoải mái lựa chọn kết hợp thông tin giữa các timestep khác nhau để tổng hợp lại và sử dụng đặc trưng tổng hợp này làm đầu vào để nhận dạng chữ cái. Cụ thể vector context được tổng hợp tại mỗi timestep như sau:</p>

<div class="row">
<span class="col-sm-12 text-center" id="aligment_model" style="font-size:150%"></span>
</div>

<p>Đầu tiên ta cần tính e với e chính là output của Aligment Model, một dạng feedforward nets với input là trạng thái của networks hiện tại</p>

<div class="row">
<span class="col-sm-12 text-center" id="aligment_score" style="font-size:150%"></span>
</div>

<p>Sau đó ta tính attention score tại mỗi timestep bằng hàm softmax vì chúng ta mong muốn tổng attention score bằng 1</p>

<div class="row">
<span class="col-sm-12 text-center" id="context_vector" style="font-size:150%"></span>
</div>

<p>Cuối cùng, context vector là weighted average của trạng thái ẩn với attention score.</p>

<script>
var alignment_model = $("#aligment_model");
katex.render("e_{ij}=a(s_{i-1}, h_{j})", alignment_model[0]);

var alignment_score = $("#aligment_score");
katex.render("\alpha_{ij} = softmax(e_{ij})", alignment_score[0]);

var context_vector = $("#context_vector");
katex.render("c_{i} =\\sum_{j=1}^{T_{x}}\alpha_{ij}*h_{j}", context_vector[0]);
</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention_rnn</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># inputs.shape = (batch_size, time_steps, input_dim)
</span>    <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Permute</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span> <span class="o">//</span> <span class="n">Alignment</span> <span class="n">Model</span> <span class="o">+</span> <span class="n">Softmax</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">K</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'dim_reduction'</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a_probs</span> <span class="o">=</span> <span class="n">Permute</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'attention_vec'</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">output_attention_mul</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">a_probs</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'attention_mul'</span><span class="p">)</span> <span class="o">//</span> <span class="n">Weighted</span> <span class="n">Average</span> 
    <span class="k">return</span> <span class="n">output_attention_mul</span>
</code></pre></div></div>
<h3 id="lstm">LSTM</h3>
<p>Với các vector context được tính ở tầng Attention được sử dụng là đầu vào cho mô hình LSTM. Tại mỗi timestep, chúng ta dự đoán từ tại thời điểm đó. Các timestep liên tục có thể dữ đoán cùng một từ.</p>
<div class="img-div">
    <img src="/images/ocr_lstm.png" />
</div>
<h3 id="connectionist-temporal-classification-loss">Connectionist Temporal Classification loss</h3>
<p>Với dữ liệu huấn luyện, chúng ta có nhãn là một đoạn text tương ứng với chữ trong bức ảnh đó. Chúng ta không có nhãn cụ thể tại mỗi thời điểm từ xuất hiện là gì tương ứng với timestep trong mô hình LSTM, do đó chúng ta không thể dùng cross entropy loss để tính độ lỗi mà phải dùng CTC loss trong bài toán bài.</p>

<h4 id="encoding-ground-truth">Encoding ground truth</h4>
<p>CTC loss giải quyết vấn đề này theo cách rất là thông minh, cụ thể chúng ta sẽ thử tất cả các alignment của ground truth và tính score của tổng tất cả alignment. Alignment của ground truth được phát sinh bằng cách thêm blank token (-) và lặp lại bất kì kí tự nào trong ground truth.</p>

<p>Ví dụ ta có ground truth là: sun và có mô hình LSTM của chúng ta dự toán 4 timesteps. Thì những alignment đúng của ground truth là:</p>
<ul>
  <li>sun -&gt; -sun, s-un, su-n, sun-</li>
  <li>sun -&gt; suun, ssun, sunn</li>
</ul>

<p>Đới với những từ có 2 kí tự liên tục giống nhau, chúng ta sẽ thêm blank token để ở giữa để tạo ra một alignment đúng. Ví dụ với kí tự too. Các alignment đúng có thể là:</p>
<ul>
  <li>too -&gt; -to-o, tto-o</li>
</ul>

<p>Nhưng ko thể là tooo.</p>
<h4 id="decoding-text">Decoding text</h4>
<p>Mô hình của chúng ta sẽ học để predict những alignment trên, sau đó chúng ta phải decode để đưa ra chuỗi dự đoán cuối cùng bằng cách gộp những kí tự lặp lại liên tiếp nhau thành một kí tự và sau đó xóa hết tất cả blank token.
Ví dụ với alignment tto-o thì sau khi decode chúng ta sẽ có too bằng cách gộp 2 kí tự ‘t’ lại với nhau và xóa ‘-‘.</p>

<h4 id="tính-ctc-loss">Tính CTC loss</h4>
<p>Với mỗi grouth truth chúng ta có nhiều alignment, bất kì alignment nào được dự đoán đều là một dự đoán đúng. Do đó, hàm loss ta cần tối ưu chính là tổng của tất cả các alignment.</p>

<div class="img-div">
    <img src="/images/ocr_ctc_loss_calc.png" />
</div>
<p>Với từ sun, ta có tổng 7 alignments đúng ở trên. Do đó theo model, xác suất từ sun xuất hiện là</p>
<div class="row">
<span class="col-sm-12 text-center">
    p('sun') = p('-sun') + p('s-un') + p('su-n') + p('sun-') + p('ssun') + p('suun') + p('sunn') = 0.2186
</span>
</div>
<p>Hàm loss của chúng ta sẽ là 1 - p(‘sun’)</p>

<h3 id="word-beam-search">Word Beam Search</h3>
<p>Chúng ta có thể lựa chọn câu được phát sinh bằng best path, hoặc có thể bằng word beam search. Đối với best path, tại mỗi thời điểm chúng ta lựa chọn từ có xác suất lớn nhất. Tuy nhiên đối với cách này,câu được phát sinh có thể không phải là câu có xác suất cao nhất. Thay vào đó,chúng ta có thể sử dụng beam search để giữ tại N câu có xác suất lớn theo theo best path rồi cuối cùng câu được chọn sẽ là câu có sác xuất cao nhất trong N câu đó.</p>

<p>Đối với bài toán OCR, mỗi timestep sẽ phát sinh một kí tự do đó từ được phát sinh có thể không nằm trong từ điển. Đối trường trường hợp này, chúng ta có thể sử dụng một số phương phát post process để xử lý câu được phát sinh. Đơn giản, chúng ta sử dụng edit distance để so sánh khoảng cách 2 từ, và thay thế từ không nằm trong từ điển bằng từ có edit distance thấp nhất. Hay phức tạp hơn, chúng ta có thể dùng language model để sửa lỗi câu được phát sinh.
Đối với python/tensorflow, các bạn có thể dùng <a href="https://github.com/githubharald/CTCDecoder">thư viện</a> sau để phát sinh câu từ model</p>

<h3 id="ocr-data-augmentation">OCR Data Augmentation</h3>
<p>Trong cuộc thi này, tập dữ liệu mà BTC cung cấp chỉ có 2000 mẫu, do đó để huấn luyện model chúng ta cần sử dụng một số phương pháp để tăng dữ liệu.</p>
<ul>
  <li>Xoay hay dịch chuyển một ít bức ảnh.</li>
  <li>Sử dụng elastic transform</li>
  <li>Random erasor một phần của bức ảnh.</li>
</ul>
<div class="img-div">
    <img src="/images/ocr_aug.png" />
</div>

<h2 id="kết-quả">Kết quả</h2>
<p>Mình train 5 folds ,mỗi fold mất khoảng 8 tiếng để chạy. Sau khi chay khoảng 70-80 epochs thì WER khoảng 0.1x. Theo mình thấy thì kết quả tương đối chính xác. Có vẻ bộ dataset hơi dễ :))</p>
<div class="img-div">
    <img src="/images/ocr_result.jpg" />
</div>

<h2 id="download-dataset">Download dataset</h2>
<p>Bộ dataset này là của <a href="http://cinnamon.is/vi/">Cinnamon</a>. Mình không chịu trách nhiệm khi các bạn sử dụng sai mục đích.</p>
<ul>
  <li><a href="https://drive.google.com/drive/folders/1Qa2YA6w6V5MaNV-qxqhsHHoYFRK5JB39">OCR</a></li>
</ul>

  </div>

  <div class="date">
    Written on October 31, 2018
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'luongvantuananh-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:tuananhhd5596@gmail.com"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/TraDa"><i class="svg-icon facebook"></i></a>

<a href="https://github.com/luongvantuananh"><i class="svg-icon github"></i></a>




<a href="https://www.twitter.com/jekyllrb"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    

  </body>
</html>
